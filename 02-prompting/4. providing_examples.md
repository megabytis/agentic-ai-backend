# Prompt Engineering Technique #3 **Providing Examples**

### (One-shot & Multi-shot Prompting)

## 1. Core Idea

LLMs learn **how you want answers** by seeing examples.

Instead of only telling the model _what_ to do, you **show it**.

This is called:

- **One-shot prompting** → 1 example
- **Multi-shot prompting** → multiple examples

It dramatically improves:

- Accuracy
- Edge-case handling
- Output formatting

---

## 2. Why This Works

Models copy patterns.
If you show:

```
Input → Output
```

The model tries to **match that transformation**.

You are teaching it the task like:

> “When input looks like this, output should look like this.”

---

## 3. Base Example (Sentiment Classification)

### Problem

Tweet:

> “yeah sure, that was the best movie I’ve ever seen since Plan 9 from Outer Space.”

Humans detect sarcasm → **Negative**
Model might misclassify as **Positive**

---

## 4. Fix Using One-Shot Prompting

Add a sample:

```xml
<example>
  <input>great game tonight</input>
  <output>positive</output>
</example>
```

This gives the model a **concrete reference** for sentiment mapping.

---

## 5. Handling Corner Cases with Multi-Shot

For sarcasm:

Add context:

> “Be especially careful with tweets that contain sarcasm.”

Then example:

```xml
<example>
  <input>oh yeah, I really needed a flight delayed tonight, excellent.</input>
  <output>negative</output>
</example>
```

Now the model understands:

- Positive words ≠ positive sentiment
- Sarcasm → likely negative

This improves classification of tricky inputs.

---

## 6. Use for Complex Output Formats

If you want:

- Structured JSON
- Detailed reports
- Custom schemas

You **must** show a full example:

```xml
<example>
  <input>...</input>
  <ideal_output>
    {
      "calories": 2200,
      "protein": "140g",
      "meals": [...]
    }
  </ideal_output>
</example>
```

Now the model **copies structure**, not just logic.

---

## 7. Using Prompt Eval Results as Examples

When running prompt evals:

1. Open the generated HTML report
2. Find the **highest scoring response**
3. Copy:
   - Input
   - Output

4. Paste into your prompt as an example

This uses **your own best data** as training signals.

---

## 8. Add Reasoning for Why It’s Ideal (Optional, Powerful)

Under your example:

```xml
<why>
This response is ideal because it is:
- Well structured
- Matches user constraints
- Uses correct quantities
- Aligns with goals
</why>
```

Now the model understands **what quality means**, not just what shape to copy.

---

## 9. Summary

| Technique            | Purpose                        |
| -------------------- | ------------------------------ |
| One-shot             | Teach basic behavior           |
| Multi-shot           | Teach edge cases + nuance      |
| Examples             | Control format + logic         |
| Corner-case examples | Reduce model confusion         |
| Eval-based examples  | Improve real-world performance |

---

## 10. When to Use This

Use examples when:

- Model is misunderstanding you
- Output format must be exact
- There are tricky edge cases
- You want consistent results

---

If you don’t use examples, you are basically hoping the model reads your mind.
It doesn’t. It copies patterns.

You now know how to program it without code.
