{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b48ac0",
   "metadata": {},
   "source": [
    "## Way: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c60707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.5-flash', contents='hi',\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6767e7",
   "metadata": {},
   "source": [
    "## Way: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dce343ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='gen-1770039876-I2h48vNiTWy70b7KJFEz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='GenAI is a kind of artificial intelligence that learns from existing data and can create new things‚Äîlike text, images, music, or code‚Äîon its own.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='The user asks: \"What is GenAI tell me in one line in simple english within 50 words ?\" They want a one line answer, simple English, within 50 words. So we need to give a single sentence, simple, under 50 words. Provide definition: \"GenAI is a type of artificial intelligence that can create new content‚Äîlike text, images, music‚Äîby learning patterns from existing data.\" That\\'s about 24 words. It\\'s one line. Should be fine.', reasoning_details=[{'format': 'unknown', 'index': 0, 'type': 'reasoning.text', 'text': 'The user asks: \"What is GenAI tell me in one line in simple english within 50 words ?\" They want a one line answer, simple English, within 50 words. So we need to give a single sentence, simple, under 50 words. Provide definition: \"GenAI is a type of artificial intelligence that can create new content‚Äîlike text, images, music‚Äîby learning patterns from existing data.\" That\\'s about 24 words. It\\'s one line. Should be fine.'}]), native_finish_reason='stop')], created=1770039877, model='openai/gpt-oss-120b:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=142, prompt_tokens=86, total_tokens=228, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=108, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), cost=0, is_byok=False, cost_details={'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}), provider='OpenInference')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain.messages import HumanMessage,AIMessage\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b:free\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is GenAI tell me in one line in simple english within 50 words ?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07202325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### The ‚ÄúHello‚ÄØWorld‚Äù of AI\n",
      "\n",
      "In every programming language the classic first program is\n",
      "\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "It proves that you can compile/run code, that the runtime works, and that you can see output.  \n",
      "For **artificial‚Äëintelligence** we want a similarly tiny program that shows:\n",
      "\n",
      "1. **A learning or reasoning step** (the ‚Äúintelligence‚Äù part).  \n",
      "2. **A visible, easy‚Äëto‚Äëunderstand result** (the ‚ÄúHello, World!‚Äù output).  \n",
      "\n",
      "Below are a few canonical ‚ÄúHello‚ÄØWorld‚Äù examples that satisfy those two goals, ranging from the ultra‚Äësimple (rule‚Äëbased) to a tiny trainable model.\n",
      "\n",
      "---\n",
      "\n",
      "## 1Ô∏è‚É£ Rule‚ÄëBased ‚ÄúChatbot‚Äù (ELIZA‚Äëstyle)\n",
      "\n",
      "The earliest AI demos were rule‚Äëbased conversational agents.  \n",
      "A one‚Äëline ‚ÄúHello, World!‚Äù for AI can be a program that **mirrors** whatever the user says.\n",
      "\n",
      "```python\n",
      "# hello_ai.py\n",
      "while True:\n",
      "    user = input(\"You: \")\n",
      "    if user.lower() in {\"exit\", \"quit\"}:\n",
      "        print(\"AI: Bye!\")\n",
      "        break\n",
      "    # Simple echo ‚Äì the AI ‚Äúunderstands‚Äù by repeating\n",
      "    print(f\"AI: You said, \\\"{user}\\\"\")\n",
      "```\n",
      "\n",
      "**Why it works as a Hello‚ÄØWorld:**\n",
      "\n",
      "- **No training data** ‚Äì the program runs immediately.  \n",
      "- **Shows interaction** ‚Äì you type, the AI responds.  \n",
      "- **Conceptually similar to ‚Äúecho‚Äù in early AI research** (e.g., ELIZA‚Äôs pattern‚Äëmatching).\n",
      "\n",
      "---\n",
      "\n",
      "## 2Ô∏è‚É£ Minimal Perceptron (learns AND)\n",
      "\n",
      "A perceptron is the simplest neural network.  \n",
      "Training it on the logical **AND** function is the classic ‚Äúfirst learning program‚Äù.\n",
      "\n",
      "```python\n",
      "# perceptron_and.py\n",
      "import numpy as np\n",
      "\n",
      "# Training data for AND\n",
      "X = np.array([[0, 0],\n",
      "              [0, 1],\n",
      "              [1, 0],\n",
      "              [1, 1]])          # inputs\n",
      "y = np.array([0, 0, 0, 1])      # desired outputs\n",
      "\n",
      "# Random weights + bias\n",
      "w = np.random.randn(2)\n",
      "b = np.random.randn()\n",
      "\n",
      "def step(z):\n",
      "    return 1 if z > 0 else 0\n",
      "\n",
      "# Simple stochastic gradient descent\n",
      "lr = 0.1\n",
      "for epoch in range(10):\n",
      "    for xi, target in zip(X, y):\n",
      "        z = np.dot(w, xi) + b\n",
      "        y_pred = step(z)\n",
      "        error = target - y_pred\n",
      "        # Update rule\n",
      "        w += lr * error * xi\n",
      "        b += lr * error\n",
      "\n",
      "print(\"Trained weights:\", w, \"bias:\", b)\n",
      "print(\"AND(1,1) =\", step(np.dot(w, [1, 1]) + b))\n",
      "```\n",
      "\n",
      "**Why it‚Äôs a Hello‚ÄØWorld for AI:**\n",
      "\n",
      "- **Training loop** ‚Äì you see the learning step in action.  \n",
      "- **Tiny dataset** ‚Äì runs instantly, no external files.  \n",
      "- **Result is interpretable** ‚Äì the network now implements a logical function.\n",
      "\n",
      "---\n",
      "\n",
      "## 3Ô∏è‚É£ One‚ÄëLayer Text Generator (tiny language model)\n",
      "\n",
      "If you want a ‚ÄúHello, World!‚Äù that actually **generates language**, a single‚Äëlayer character‚Äëlevel model does the trick.\n",
      "\n",
      "```python\n",
      "# tiny_char_rnn.py\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "# Sample text (tiny corpus)\n",
      "corpus = \"hello world! \"\n",
      "\n",
      "# Build character vocab\n",
      "chars = sorted(set(corpus))\n",
      "stoi = {c:i for i,c in enumerate(chars)}\n",
      "itos = {i:c for c,i in stoi.items()}\n",
      "vocab_size = len(chars)\n",
      "\n",
      "# Encode the whole corpus\n",
      "data = torch.tensor([stoi[c] for c in corpus], dtype=torch.long)\n",
      "\n",
      "# Hyper‚Äëparameters\n",
      "embed_dim = 8\n",
      "hidden_dim = 16\n",
      "seq_len = 5   # context length\n",
      "\n",
      "class CharRNN(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
      "        self.rnn   = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
      "        self.fc    = nn.Linear(hidden_dim, vocab_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.embed(x)\n",
      "        out, _ = self.rnn(x)\n",
      "        logits = self.fc(out[:, -1])   # predict next char from last hidden state\n",
      "        return logits\n",
      "\n",
      "model = CharRNN()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
      "\n",
      "# Training loop (very few steps)\n",
      "for epoch in range(200):\n",
      "    # Randomly pick a start index\n",
      "    start = torch.randint(0, len(data)-seq_len-1, (1,)).item()\n",
      "    x = data[start:start+seq_len].unsqueeze(0)   # (1, seq_len)\n",
      "    y = data[start+seq_len]                      # target char index\n",
      "\n",
      "    optimizer.zero_grad()\n",
      "    logits = model(x)\n",
      "    loss = criterion(logits, y.unsqueeze(0))\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "    if epoch % 50 == 0:\n",
      "        print(f\"epoch {epoch:3d} loss {loss.item():.4f}\")\n",
      "\n",
      "# Generation (seed = \"h\")\n",
      "seed = \"h\"\n",
      "generated = seed\n",
      "model.eval()\n",
      "with torch.no_grad():\n",
      "    for _ in range(20):\n",
      "        idx = torch.tensor([stoi[c] for c in generated[-seq_len:]], dtype=torch.long).unsqueeze(0)\n",
      "        logits = model(idx)\n",
      "        probs = torch.softmax(logits, dim=-1)\n",
      "        next_idx = torch.multinomial(probs, num_samples=1).item()\n",
      "        generated += itos[next_idx]\n",
      "\n",
      "print(\"\\nGenerated text:\", generated)\n",
      "```\n",
      "\n",
      "**Why this is a good AI ‚ÄúHello‚ÄØWorld‚Äù:**\n",
      "\n",
      "- **Shows a learning algorithm (gradient descent) on real data** (the tiny corpus).  \n",
      "- **Produces new text** ‚Äì you can see the model‚Äôs ‚Äúcreativity‚Äù.  \n",
      "- **All code fits on a single screen** and runs in seconds on a CPU.\n",
      "\n",
      "---\n",
      "\n",
      "## 4Ô∏è‚É£ Reinforcement‚ÄëLearning Mini‚ÄëGame (CartPole ‚ÄúHello, World!‚Äù)\n",
      "\n",
      "In RL, the canonical first environment is **OpenAI Gym‚Äôs CartPole**.  \n",
      "A minimal agent that learns to keep the pole upright for a few episodes is the RL analogue of ‚ÄúHello, World!‚Äù.\n",
      "\n",
      "```python\n",
      "# cartpole_hello.py\n",
      "import gym\n",
      "import numpy as np\n",
      "\n",
      "env = gym.make(\"CartPole-v1\")\n",
      "obs_dim = env.observation_space.shape[0]\n",
      "n_actions = env.action_space.n\n",
      "\n",
      "# Very simple linear policy: a = sign(w¬∑obs)\n",
      "w = np.random.randn(obs_dim)\n",
      "\n",
      "def policy(obs):\n",
      "    return 0 if np.dot(w, obs) < 0 else 1\n",
      "\n",
      "# REINFORCE‚Äëstyle update (one‚Äëstep policy gradient)\n",
      "lr = 1e-2\n",
      "for episode in range(200):\n",
      "    obs = env.reset()\n",
      "    done = False\n",
      "    total_reward = 0\n",
      "    grads = []\n",
      "    while not done:\n",
      "        a = policy(obs)\n",
      "        # Gradient of log‚Äëpolicy for linear sign policy:\n",
      "        grad = (1 - 2*a) * obs   # +1 for action 0, -1 for action 1\n",
      "        grads.append(grad)\n",
      "\n",
      "        obs, reward, done, _ = env.step(a)\n",
      "        total_reward += reward\n",
      "\n",
      "    # Policy gradient update (sum of grads * total reward)\n",
      "    w += lr * total_reward * np.sum(grads, axis=0)\n",
      "\n",
      "    if episode % 20 == 0:\n",
      "        print(f\"Episode {episode:3d}  reward {total_reward}\")\n",
      "\n",
      "env.close()\n",
      "```\n",
      "\n",
      "**Why it works as a Hello‚ÄØWorld:**\n",
      "\n",
      "- **Interacts with a simulated environment** ‚Äì the AI *acts* and *receives feedback*.  \n",
      "- **One‚Äëline policy** (linear + sign) makes the math transparent.  \n",
      "- **Training finishes in seconds** and you can watch the reward climb from ~10 to >150 (the ‚Äúsolved‚Äù threshold).\n",
      "\n",
      "---\n",
      "\n",
      "## 5Ô∏è‚É£ Quick‚ÄëLook at Modern ‚ÄúHello‚ÄØWorld‚Äù ‚Äì Prompt‚ÄëBased LLM\n",
      "\n",
      "If you already have access to a large language model (e.g., via OpenAI‚Äôs API), the simplest AI program is just a **prompt**:\n",
      "\n",
      "```python\n",
      "import openai\n",
      "\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=\"gpt-4o-mini\",\n",
      "    messages=[{\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
      "              {\"role\": \"user\",   \"content\": \"Say hello in three languages!\"}]\n",
      ")\n",
      "\n",
      "print(response.choices[0].message.content)\n",
      "```\n",
      "\n",
      "**Why this is the modern equivalent:**\n",
      "\n",
      "- **Zero‚Äëtraining** ‚Äì the model already knows language.  \n",
      "- **Shows the core AI interaction pattern:** *send a prompt ‚Üí receive a response*.  \n",
      "- **Works for any ‚ÄúHello, World!‚Äù style task** (translation, summarization, code generation, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "## üìö TL;DR ‚Äì Pick the one that matches your curiosity\n",
      "\n",
      "| Goal | Minimal code | Shows learning? | Shows interaction? |\n",
      "|------|--------------|-----------------|--------------------|\n",
      "| **Rule‚Äëbased echo** | 10‚ÄØlines | No | ‚úÖ |\n",
      "| **Perceptron (AND)** | 20‚ÄØlines | ‚úÖ | No |\n",
      "| **Tiny character RNN** | ~40‚ÄØlines | ‚úÖ | ‚úÖ |\n",
      "| **CartPole RL** | ~30‚ÄØlines | ‚úÖ | ‚úÖ (environment) |\n",
      "| **LLM prompt** | 5‚ÄØlines (API) | No (pre‚Äëtrained) | ‚úÖ |\n",
      "\n",
      "All of these snippets are self‚Äëcontained, run in a few seconds on a laptop, and illustrate the *essence* of an AI system: **input ‚Üí processing (often learning) ‚Üí output**. Pick the one that best fits the level of ‚ÄúAI‚Äëness‚Äù you want to showcase, and you‚Äôve got your **Hello‚ÄØWorld** for artificial intelligence!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"openai/gpt-oss-120b:free\",\n",
    "    # instructions=\"You are a coding assistant that talks like a pirate.\",\n",
    "    input=\"What would be the AI equivalent of Hello World?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
