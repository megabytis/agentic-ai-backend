{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19cc8ebd",
   "metadata": {},
   "source": [
    "# Complete RAG Flow - 7 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c45f8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15713bcf",
   "metadata": {},
   "source": [
    "## STEP 1: Chunking by Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "112095c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_section(text):\n",
    "    \"\"\"Split document by markdown headers (##)\"\"\"\n",
    "    pattern = r\"\\n## \"\n",
    "    chunks = re.split(pattern, text)\n",
    "    return [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "\n",
    "with open('./report.md','r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chunks = chunk_by_section(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55806bf",
   "metadata": {},
   "source": [
    "## STEP 2: Getting Embeddings for each chunk using fastembd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "699fefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TextEmbedding()\n",
    "\n",
    "\n",
    "def get_embedding(texts):\n",
    "    return list(embedding_model.embed(texts))\n",
    "\n",
    "\n",
    "chunk_embeddings = [get_embedding(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9913ec9",
   "metadata": {},
   "source": [
    "## STEP 3: Creating Vector store & inserting all chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1952193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorIndex:\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # store embeddings\n",
    "        self.metadata = []  # store original chunks with metadata\n",
    "\n",
    "    def add_vector(self, embeddings, metadata):\n",
    "        \"\"\"Adding a vector and it's metadata to the index\"\"\"\n",
    "        self.vectors.append(np.array(embeddings).flatten()) # used 'flatten()' to force convert all incoming embeddings to 1D array\n",
    "        self.metadata.append(metadata)\n",
    "\n",
    "    def search(self, query_embedding, top_k=2):\n",
    "        \"\"\"Find most similar vectors using cosine similarity\"\"\"\n",
    "        # Convert to numpy for faster computation\n",
    "        query_emb = np.array(query_embedding)\n",
    "\n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, vec in enumerate(self.vectors):\n",
    "            vec = np.array(vec)\n",
    "            # Cosine similarity formula\n",
    "            similarity = np.dot(query_emb, vec) / (\n",
    "                np.linalg.norm(query_emb) * np.linalg.norm(vec)\n",
    "            )\n",
    "            similarities.append((similarity, i))\n",
    "\n",
    "        # Sort by similarity (highest first)\n",
    "        similarities.sort(reverse=True)\n",
    "\n",
    "        # Return top_k results\n",
    "        results = []\n",
    "        for sim, idx in similarities[:top_k]:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"distance\": 1 - sim,  # Convert similarity to distance (like video)\n",
    "                    \"content\": self.metadata[idx][\"content\"],\n",
    "                    \"similarity\": sim,\n",
    "                }\n",
    "            )\n",
    "        return results\n",
    "\n",
    "\n",
    "store = VectorIndex()\n",
    "\n",
    "# Now looping through each chunk and it's embeddings\n",
    "for embedding, chunk in zip(chunk_embeddings, chunks):\n",
    "    store.add_vector(embedding, {\"content\": chunk})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e7514",
   "metadata": {},
   "source": [
    "## STEP 4: User asks a question (create embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52093426",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_qn = \"What did the software engineering department do last year?\"\n",
    "question_embedding = list(embedding_model.embed([user_qn]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109254c2",
   "metadata": {},
   "source": [
    "## STPE 5: searching for most relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edfbfa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Result 1:\n",
      "    Distance: 0.349\n",
      "    Content: Methodology\n",
      "\n",
      "The insights compiled within this Annual Interdisciplinary Research Review represent a synthesis of findings drawn from standard departme...\n",
      "\n",
      "    Result 2:\n",
      "    Distance: 0.366\n",
      "    Content: Executive Summary\n",
      "\n",
      "This report synthesizes the key findings and ongoing research efforts across the organization's diverse operational and R&D departm...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = store.search(question_embedding,top_k=2)\n",
    "\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"    Result {i+1}:\")\n",
    "    print(f\"    Distance: {result['distance']:.3f}\")\n",
    "    print(f\"    Content: {result['content'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a50cf",
   "metadata": {},
   "source": [
    "## STEP 6: Building the final prompt for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28a0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n---\\n\\n\".join([r['content'] for r in results])\n",
    "\n",
    "prompt = f\"\"\"You are a helpful assistant answering questions based on provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER QUESTION:\n",
    "{user_qn}\n",
    "\n",
    "Answer the question based ONLY on the context provided. If the answer cannot be found in the context, say \"I cannot find this information in the provided documents.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24f6a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=os.getenv(\"OPENROUTER_API_KEY\")\n",
    "BASE_URL=\"https://openrouter.ai/api/v1\"\n",
    "MODEL=\"stepfun/step-3.5-flash:free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9a3014a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the software engineering department tackled persistent stability issues and implemented key fixes identified through error code analysis, with a specific example being the error code `ERR_MEM_ALLOC_FAIL_0x8007000E`.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_llm_ans(prompt):\n",
    "    client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=BASE_URL\n",
    "    )\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "ans = get_llm_ans(prompt)\n",
    "ans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
